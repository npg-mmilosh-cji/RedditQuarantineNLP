{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmilosh/Library/Python/3.8/lib/python/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.corpora as corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_selftext</th>\n",
       "      <th>has_long_token</th>\n",
       "      <th>text_clean_space</th>\n",
       "      <th>text_clean_punc_lower</th>\n",
       "      <th>len_clean</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>tokens_lemma</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2490481</th>\n",
       "      <td>Even if they hate Trump they could at least ac...</td>\n",
       "      <td>False</td>\n",
       "      <td>Even if they hate Trump they could at least ac...</td>\n",
       "      <td>even if they hate trump they could at least ac...</td>\n",
       "      <td>139</td>\n",
       "      <td>[even, if, they, hate, trump, they, could, at,...</td>\n",
       "      <td>[even, hate, trump, could, least, acknowledge,...</td>\n",
       "      <td>[even, hate, trump, could, least, acknowledge,...</td>\n",
       "      <td>[(even, hate), (hate, trump), (trump, could), ...</td>\n",
       "      <td>[(even, hate, trump), (hate, trump, could), (t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490482</th>\n",
       "      <td>Heterosexual reproduction is a mystery to admi...</td>\n",
       "      <td>False</td>\n",
       "      <td>Heterosexual reproduction is a mystery to admi...</td>\n",
       "      <td>heterosexual reproduction is a mystery to admi...</td>\n",
       "      <td>76</td>\n",
       "      <td>[heterosexual, reproduction, is, a, mystery, t...</td>\n",
       "      <td>[heterosexual, reproduction, mystery, admins, ...</td>\n",
       "      <td>[heterosexual, reproduction, mystery, admins, ...</td>\n",
       "      <td>[(heterosexual, reproduction), (reproduction, ...</td>\n",
       "      <td>[(heterosexual, reproduction, mystery), (repro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490483</th>\n",
       "      <td>Were you expecting the actual pope?</td>\n",
       "      <td>False</td>\n",
       "      <td>Were you expecting the actual pope?</td>\n",
       "      <td>were you expecting the actual pope</td>\n",
       "      <td>34</td>\n",
       "      <td>[were, you, expecting, the, actual, pope]</td>\n",
       "      <td>[expecting, actual, pope]</td>\n",
       "      <td>[expecting, actual, pope]</td>\n",
       "      <td>[(expecting, actual), (actual, pope)]</td>\n",
       "      <td>[(expecting, actual, pope)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490484</th>\n",
       "      <td>Be fruitful and multiply, not fruity and blow ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Be fruitful and multiply, not fruity and blow ...</td>\n",
       "      <td>be fruitful and multiply not fruity and blow a...</td>\n",
       "      <td>50</td>\n",
       "      <td>[be, fruitful, and, multiply, not, fruity, and...</td>\n",
       "      <td>[fruitful, multiply, fruity, blow, guy]</td>\n",
       "      <td>[fruitful, multiply, fruity, blow, guy]</td>\n",
       "      <td>[(fruitful, multiply), (multiply, fruity), (fr...</td>\n",
       "      <td>[(fruitful, multiply, fruity), (multiply, frui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490485</th>\n",
       "      <td>The term has completely lost all meaning. It's...</td>\n",
       "      <td>False</td>\n",
       "      <td>The term has completely lost all meaning. It's...</td>\n",
       "      <td>the term has completely lost all meaning its j...</td>\n",
       "      <td>104</td>\n",
       "      <td>[the, term, has, completely, lost, all, meanin...</td>\n",
       "      <td>[term, completely, lost, meaning, generalizati...</td>\n",
       "      <td>[term, completely, lost, meaning, generalizati...</td>\n",
       "      <td>[(term, completely), (completely, lost), (lost...</td>\n",
       "      <td>[(term, completely, lost), (completely, lost, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             orig_selftext  has_long_token  \\\n",
       "2490481  Even if they hate Trump they could at least ac...           False   \n",
       "2490482  Heterosexual reproduction is a mystery to admi...           False   \n",
       "2490483                Were you expecting the actual pope?           False   \n",
       "2490484  Be fruitful and multiply, not fruity and blow ...           False   \n",
       "2490485  The term has completely lost all meaning. It's...           False   \n",
       "\n",
       "                                          text_clean_space  \\\n",
       "2490481  Even if they hate Trump they could at least ac...   \n",
       "2490482  Heterosexual reproduction is a mystery to admi...   \n",
       "2490483                Were you expecting the actual pope?   \n",
       "2490484  Be fruitful and multiply, not fruity and blow ...   \n",
       "2490485  The term has completely lost all meaning. It's...   \n",
       "\n",
       "                                     text_clean_punc_lower  len_clean  \\\n",
       "2490481  even if they hate trump they could at least ac...        139   \n",
       "2490482  heterosexual reproduction is a mystery to admi...         76   \n",
       "2490483                 were you expecting the actual pope         34   \n",
       "2490484  be fruitful and multiply not fruity and blow a...         50   \n",
       "2490485  the term has completely lost all meaning its j...        104   \n",
       "\n",
       "                                                    tokens  \\\n",
       "2490481  [even, if, they, hate, trump, they, could, at,...   \n",
       "2490482  [heterosexual, reproduction, is, a, mystery, t...   \n",
       "2490483          [were, you, expecting, the, actual, pope]   \n",
       "2490484  [be, fruitful, and, multiply, not, fruity, and...   \n",
       "2490485  [the, term, has, completely, lost, all, meanin...   \n",
       "\n",
       "                                              tokens_clean  \\\n",
       "2490481  [even, hate, trump, could, least, acknowledge,...   \n",
       "2490482  [heterosexual, reproduction, mystery, admins, ...   \n",
       "2490483                          [expecting, actual, pope]   \n",
       "2490484            [fruitful, multiply, fruity, blow, guy]   \n",
       "2490485  [term, completely, lost, meaning, generalizati...   \n",
       "\n",
       "                                              tokens_lemma  \\\n",
       "2490481  [even, hate, trump, could, least, acknowledge,...   \n",
       "2490482  [heterosexual, reproduction, mystery, admins, ...   \n",
       "2490483                          [expecting, actual, pope]   \n",
       "2490484            [fruitful, multiply, fruity, blow, guy]   \n",
       "2490485  [term, completely, lost, meaning, generalizati...   \n",
       "\n",
       "                                                   bigrams  \\\n",
       "2490481  [(even, hate), (hate, trump), (trump, could), ...   \n",
       "2490482  [(heterosexual, reproduction), (reproduction, ...   \n",
       "2490483              [(expecting, actual), (actual, pope)]   \n",
       "2490484  [(fruitful, multiply), (multiply, fruity), (fr...   \n",
       "2490485  [(term, completely), (completely, lost), (lost...   \n",
       "\n",
       "                                                  trigrams  \n",
       "2490481  [(even, hate, trump), (hate, trump, could), (t...  \n",
       "2490482  [(heterosexual, reproduction, mystery), (repro...  \n",
       "2490483                        [(expecting, actual, pope)]  \n",
       "2490484  [(fruitful, multiply, fruity), (multiply, frui...  \n",
       "2490485  [(term, completely, lost), (completely, lost, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_path = 'sampled_processed_extant_posts_june_july.pkl'\n",
    "posts_df = pd.read_pickle(pkl_path)\n",
    "posts_df.head()\n",
    "# need to delete 'BotForceOne', 'AutoModerator'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = posts_df['tokens_lemma']\n",
    "del posts_df\n",
    "\n",
    "id2word = corpora.Dictionary(tokens)\n",
    "corpus = [id2word.doc2bow(text) for text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    trained_models = dict()\n",
    "    for num_topics in range(5, 30, 2):\n",
    "        model_path = path_to_box + 'lda_models/lda_' + str(num_topics) + '_topics'\n",
    "        print(\"Loading LDA(k=%d) from %s\" % (num_topics, model_path))\n",
    "        trained_models[num_topics] = gensim.models.LdaMulticore.load(model_path)\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write models\n",
    "# Won't keep results in memory\n",
    "\n",
    "path_to_box = '' # to RedditQuarantineNLP\n",
    "for num_topics in range(5, 30, 2):\n",
    "    print(\"k = \" + str(num_topics))\n",
    "    lda = gensim.models.LdaMulticore(corpus = corpus, id2word = id2word,\n",
    "                                     num_topics = num_topics, workers = 3, # use more if you can (#cores - 1)\n",
    "                                     iterations=100, random_state=0, eval_every=None)\n",
    "    model_path = path_to_box + 'lda_models/lda_' + str(num_topics) + '_topics'\n",
    "    lda.save(model_path, separately = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked by average 'c_v' coherence:\n",
      "\n",
      "num_topics=7:\t0.6971\n",
      "num_topics=9:\t0.6918\n",
      "num_topics=5:\t0.6888\n",
      "num_topics=13:\t0.6794\n",
      "num_topics=11:\t0.6667\n",
      "num_topics=15:\t0.6618\n",
      "num_topics=17:\t0.6452\n",
      "num_topics=19:\t0.6381\n",
      "num_topics=23:\t0.6373\n",
      "num_topics=27:\t0.6319\n",
      "num_topics=21:\t0.6254\n",
      "num_topics=29:\t0.6039\n",
      "num_topics=25:\t0.5975\n",
      "\n",
      "Best: 7\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "trained_models = dict()\n",
    "for num_topics in range(5, 30, 2):\n",
    "    model_path = path_to_box + 'lda_models/lda_' + str(num_topics) + '_topics'\n",
    "    print(\"Loading LDA from %s\" % (model_path))\n",
    "    trained_models[num_topics] = gensim.models.LdaMulticore.load(model_path)\n",
    "\n",
    "# Coherence\n",
    "cm = gensim.models.CoherenceModel.for_models(\n",
    "    trained_models.values(), id2word, texts = tokens, coherence = 'c_v')\n",
    "\n",
    "coherence_estimates = cm.compare_models(trained_models.values())\n",
    "coherences = dict(zip(trained_models.keys(), coherence_estimates))\n",
    "\n",
    "def print_coherence_rankings(coherences):\n",
    "    avg_coherence = \\\n",
    "        [(num_topics, avg_coherence)\n",
    "         for num_topics, (_, avg_coherence) in coherences.items()]\n",
    "    ranked = sorted(avg_coherence, key=lambda tup: tup[1], reverse=True)\n",
    "    print(\"Ranked by average '%s' coherence:\\n\" % cm.coherence)\n",
    "    for item in ranked:\n",
    "        print(\"num_topics=%d:\\t%.4f\" % item)\n",
    "    print(\"\\nBest: %d\" % ranked[0][0])\n",
    "    \n",
    "print_coherence_rankings(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  '0.048*\"woman\" + 0.048*\"white\" + 0.036*\"gay\" + 0.030*\"black\" + 0.024*\"men\" + 0.024*\"people\" + 0.023*\"hate\" + 0.022*\"racist\" + 0.017*\"muslim\" + 0.014*\"dude\"'),\n",
       " (8,\n",
       "  '0.056*\"automatically\" + 0.045*\"trump\" + 0.026*\"president\" + 0.021*\"watch\" + 0.013*\"submission\" + 0.013*\"week\" + 0.013*\"youtube\" + 0.011*\"time\" + 0.010*\"clinton\" + 0.010*\"donald\"'),\n",
       " (0,\n",
       "  '0.064*\"good\" + 0.025*\"make\" + 0.018*\"look\" + 0.017*\"win\" + 0.014*\"trump\" + 0.013*\"sad\" + 0.012*\"really\" + 0.011*\"sense\" + 0.010*\"wow\" + 0.010*\"hope\"'),\n",
       " (1,\n",
       "  '0.026*\"u\" + 0.022*\"country\" + 0.022*\"child\" + 0.020*\"kid\" + 0.015*\"family\" + 0.014*\"law\" + 0.012*\"police\" + 0.010*\"home\" + 0.010*\"parent\" + 0.010*\"back\"'),\n",
       " (4,\n",
       "  '0.027*\"right\" + 0.013*\"people\" + 0.013*\"free\" + 0.011*\"rule\" + 0.011*\"want\" + 0.009*\"speech\" + 0.008*\"community\" + 0.007*\"school\" + 0.007*\"law\" + 0.007*\"place\"'),\n",
       " (2,\n",
       "  '0.026*\"read\" + 0.017*\"baby\" + 0.013*\"biden\" + 0.011*\"article\" + 0.011*\"story\" + 0.011*\"kill\" + 0.010*\"hear\" + 0.010*\"joe\" + 0.009*\"one\" + 0.007*\"haha\"'),\n",
       " (17,\n",
       "  '0.041*\"lol\" + 0.019*\"believe\" + 0.018*\"fake\" + 0.015*\"people\" + 0.015*\"approval\" + 0.015*\"liberal\" + 0.011*\"death\" + 0.011*\"news\" + 0.011*\"know\" + 0.010*\"idiot\"'),\n",
       " (14,\n",
       "  '0.082*\"post\" + 0.058*\"comment\" + 0.048*\"please\" + 0.047*\"question\" + 0.043*\"moderator\" + 0.035*\"use\" + 0.035*\"action\" + 0.032*\"concern\" + 0.030*\"contact\" + 0.030*\"posted\"'),\n",
       " (5,\n",
       "  '0.028*\"people\" + 0.020*\"think\" + 0.017*\"great\" + 0.014*\"make\" + 0.013*\"want\" + 0.012*\"america\" + 0.011*\"u\" + 0.011*\"really\" + 0.010*\"much\" + 0.010*\"work\"'),\n",
       " (18,\n",
       "  '0.025*\"money\" + 0.020*\"pay\" + 0.016*\"tax\" + 0.013*\"government\" + 0.010*\"state\" + 0.009*\"company\" + 0.008*\"case\" + 0.007*\"cost\" + 0.007*\"court\" + 0.006*\"paid\"'),\n",
       " (13,\n",
       "  '0.021*\"interfering\" + 0.019*\"flag\" + 0.018*\"time\" + 0.015*\"cnn\" + 0.012*\"brother\" + 0.012*\"clown\" + 0.011*\"call\" + 0.011*\"show\" + 0.009*\"tweet\" + 0.009*\"one\"'),\n",
       " (15,\n",
       "  '0.058*\"fuck\" + 0.048*\"god\" + 0.031*\"gt\" + 0.029*\"oh\" + 0.029*\"thank\" + 0.015*\"game\" + 0.012*\"team\" + 0.011*\"fun\" + 0.011*\"play\" + 0.011*\"beautiful\"'),\n",
       " (10,\n",
       "  '0.036*\"ive\" + 0.034*\"man\" + 0.033*\"state\" + 0.031*\"wall\" + 0.029*\"shit\" + 0.022*\"seen\" + 0.022*\"bad\" + 0.021*\"ever\" + 0.019*\"sure\" + 0.017*\"never\"'),\n",
       " (24,\n",
       "  '0.020*\"people\" + 0.016*\"medium\" + 0.015*\"trump\" + 0.009*\"party\" + 0.009*\"left\" + 0.009*\"think\" + 0.008*\"thing\" + 0.007*\"political\" + 0.007*\"social\" + 0.007*\"leftist\"'),\n",
       " (19,\n",
       "  '0.095*\"stop\" + 0.076*\"cant\" + 0.062*\"million\" + 0.051*\"pic\" + 0.045*\"take\" + 0.039*\"10\" + 0.035*\"protip\" + 0.029*\"folk\" + 0.026*\"year\" + 0.023*\"yep\"'),\n",
       " (16,\n",
       "  '0.067*\"go\" + 0.043*\"back\" + 0.017*\"iran\" + 0.011*\"dick\" + 0.011*\"came\" + 0.008*\"uk\" + 0.008*\"jail\" + 0.007*\"let\" + 0.007*\"op\" + 0.007*\"one\"'),\n",
       " (23,\n",
       "  '0.061*\"year\" + 0.028*\"old\" + 0.022*\"thanks\" + 0.021*\"day\" + 0.015*\"ago\" + 0.015*\"month\" + 0.010*\"20\" + 0.010*\"boy\" + 0.010*\"last\" + 0.010*\"mental\"'),\n",
       " (12,\n",
       "  '0.037*\"ampx200b\" + 0.025*\"american\" + 0.023*\"violence\" + 0.015*\"people\" + 0.011*\"low\" + 0.011*\"slave\" + 0.011*\"fire\" + 0.010*\"camp\" + 0.010*\"history\" + 0.010*\"sick\"'),\n",
       " (22,\n",
       "  '0.034*\"vote\" + 0.034*\"democrat\" + 0.024*\"say\" + 0.014*\"damn\" + 0.012*\"trump\" + 0.011*\"republican\" + 0.011*\"dems\" + 0.010*\"fucking\" + 0.010*\"bitch\" + 0.010*\"know\"'),\n",
       " (11,\n",
       "  '0.052*\"reddit\" + 0.049*\"brick\" + 0.039*\"user\" + 0.033*\"sub\" + 0.030*\"every\" + 0.021*\"admins\" + 0.016*\"get\" + 0.015*\"outside\" + 0.014*\"starting\" + 0.014*\"td\"')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models[25].print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
